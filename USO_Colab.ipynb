{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üé® USO: Unified Style and Subject-Driven Generation\n",
        "\n",
        "[![GitHub](https://img.shields.io/static/v1?label=GitHub&message=Code&color=green&logo=github)](https://github.com/bytedance/USO)\n",
        "[![Project Page](https://img.shields.io/badge/Project%20Page-USO-yellow)](https://bytedance.github.io/USO/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv%20paper-USO-b31b1b.svg)](https://arxiv.org/abs/2508.18966)\n",
        "[![Hugging Face Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=orange)](https://huggingface.co/bytedance-research/USO)\n",
        "\n",
        "This notebook runs the **USO Gradio Web App** in Google Colab with GPU acceleration.\n",
        "\n",
        "**USO** is a unified framework for style-driven and subject-driven image generation that can freely combine any subjects with any styles in any scenarios.\n",
        "\n",
        "## Features\n",
        "- **Subject/Identity-driven generation**: Place subjects into new scenes\n",
        "- **Style-driven generation**: Generate images matching a given style\n",
        "- **Style-subject driven generation**: Combine content and style references\n",
        "- **Multi-style generation**: Blend multiple style references\n",
        "\n",
        "---\n",
        "‚ö†Ô∏è **Important**: This notebook requires a GPU runtime. Go to **Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or higher)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "First, we'll clone the repository and install the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone the USO repository\n",
        "!git clone https://github.com/bytedance/USO.git\n",
        "%cd USO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install project dependencies\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf_token_section"
      },
      "source": [
        "## 2. Configure Hugging Face Token\n",
        "\n",
        "You need a Hugging Face token to download the FLUX.1-dev model weights.\n",
        "\n",
        "1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens) and create a token\n",
        "2. Accept the [FLUX.1-dev license](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
        "3. Enter your token below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_token_input"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Securely input your Hugging Face token\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Write the .env file\n",
        "env_content = f\"\"\"# Hugging face token\n",
        "HF_TOKEN={HF_TOKEN}\n",
        "\n",
        "# Core Flux weights\n",
        "FLUX_DEV=./weights/FLUX.1-dev/flux1-dev.safetensors\n",
        "FLUX_DEV_FP8=./weights/FLUX.1-dev/flux1-dev.safetensors\n",
        "AE=./weights/FLUX.1-dev/ae.safetensors\n",
        "\n",
        "# Text + vision encoders\n",
        "T5=./weights/t5-xxl\n",
        "CLIP=./weights/clip-vit-l14\n",
        "LORA=./weights/USO/uso_flux_v1.0/dit_lora.safetensors\n",
        "\n",
        "# USO LoRA + projector\n",
        "PROJECTION_MODEL=./weights/USO/uso_flux_v1.0/projector.safetensors\n",
        "SIGLIP_PATH=./weights/siglip\n",
        "\"\"\"\n",
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"‚úÖ .env file created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## 3. Download Model Weights\n",
        "\n",
        "This will download all necessary model weights from Hugging Face. This may take several minutes depending on your connection speed.\n",
        "\n",
        "**Models being downloaded:**\n",
        "- FLUX.1-dev (main diffusion model)\n",
        "- USO LoRA weights and projector\n",
        "- T5-XXL (text encoder)\n",
        "- CLIP ViT-L/14 (text encoder)\n",
        "- SigLIP (vision encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_weights"
      },
      "outputs": [],
      "source": [
        "# Download all required model weights\n",
        "!python ./weights/downloader.py\n",
        "\n",
        "print(\"\\n‚úÖ All model weights downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "launch_section"
      },
      "source": [
        "## 4. Launch the Gradio Web App\n",
        "\n",
        "Now we'll launch the USO Gradio interface. The app will be accessible via a public URL.\n",
        "\n",
        "**Options:**\n",
        "- `--offload`: Enables sequential offloading of models to CPU when not in use (reduces VRAM usage)\n",
        "- `--name flux-dev-fp8`: Uses FP8 quantized model for lower memory usage (~16-18GB VRAM)\n",
        "\n",
        "For Colab T4 GPU (16GB VRAM), we use the memory-efficient configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_app"
      },
      "outputs": [],
      "source": [
        "# Launch the Gradio app with public sharing enabled\n",
        "# Using offload mode for lower VRAM usage on Colab\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Run the app in the background\n",
        "!python app.py --offload --name flux-dev-fp8 --port 7860 &\n",
        "\n",
        "# Wait for the server to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Create a public URL using localtunnel\n",
        "!npm install -g localtunnel\n",
        "!npx localtunnel --port 7860"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alternative_launch"
      },
      "source": [
        "### Alternative: Launch with Gradio's Built-in Sharing\n",
        "\n",
        "If localtunnel doesn't work, you can modify `app.py` to enable Gradio's built-in sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio_share"
      },
      "outputs": [],
      "source": [
        "# Alternative method: Enable Gradio sharing directly\n",
        "# This modifies app.py to use share=True\n",
        "\n",
        "import re\n",
        "\n",
        "# Read the original app.py\n",
        "with open(\"app.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Modify the launch line to include share=True\n",
        "content = content.replace(\n",
        "    \"demo.launch(server_port=args.port)\",\n",
        "    \"demo.launch(server_port=args.port, share=True)\"\n",
        ")\n",
        "\n",
        "# Write the modified app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ Modified app.py to enable public sharing\")\n",
        "print(\"\\nüöÄ Now run the cell below to start the app with a public Gradio URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_shared_app"
      },
      "outputs": [],
      "source": [
        "# Run the app with Gradio sharing enabled\n",
        "!python app.py --offload --name flux-dev-fp8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_tips"
      },
      "source": [
        "## üìù Usage Tips\n",
        "\n",
        "### Model Supports 3 Types of Usage:\n",
        "\n",
        "**1. Only Content Image (Subject/Identity-driven)**\n",
        "- Use natural prompts like \"A dog on the beach\" or \"The woman near the sea\"\n",
        "- For style editing: \"Transform the image into Ghibli style\"\n",
        "\n",
        "**2. Only Style Image (Style-driven)**\n",
        "- Upload one or more style reference images\n",
        "- Use any prompt to generate content in that style\n",
        "\n",
        "**3. Content + Style Images (Style-subject driven)**\n",
        "- Layout-preserved: Set prompt to **empty**\n",
        "- Layout-shifted: Use a natural prompt\n",
        "\n",
        "### Best Practices:\n",
        "- For portraits, use half-body close-ups for half-body prompts\n",
        "- Use full-body images when the pose changes significantly\n",
        "- The model is trained on 1024x1024 resolution\n",
        "\n",
        "---\n",
        "\n",
        "‚≠ê If you find USO helpful, please star the [GitHub repository](https://github.com/bytedance/USO)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM) Errors\n",
        "- Make sure you're using `--offload --name flux-dev-fp8` flags\n",
        "- Reduce the image resolution (e.g., 768x768 instead of 1024x1024)\n",
        "- Restart the runtime to clear GPU memory\n",
        "\n",
        "### Model Download Issues\n",
        "- Verify your Hugging Face token is correct\n",
        "- Make sure you've accepted the FLUX.1-dev license on Hugging Face\n",
        "- Check your internet connection\n",
        "\n",
        "### GPU Not Detected\n",
        "- Go to **Runtime ‚Üí Change runtime type ‚Üí GPU**\n",
        "- If no GPU is available, wait and try again later"
      ]
    }
  ]
}
