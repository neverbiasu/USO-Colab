{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üé® USO: Unified Style and Subject-Driven Generation\n",
        "\n",
        "[![GitHub](https://img.shields.io/static/v1?label=GitHub&message=Code&color=green&logo=github)](https://github.com/bytedance/USO)\n",
        "[![Project Page](https://img.shields.io/badge/Project%20Page-USO-yellow)](https://bytedance.github.io/USO/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv%20paper-USO-b31b1b.svg)](https://arxiv.org/abs/2508.18966)\n",
        "[![Hugging Face Model](https://img.shields.io/static/v1?label=%F0%9F%A4%97%20Hugging%20Face&message=Model&color=orange)](https://huggingface.co/bytedance-research/USO)\n",
        "[![Follow @neverbiasu](https://img.shields.io/github/followers/neverbiasu?label=Follow%20%40neverbiasu&style=social)](https://github.com/neverbiasu)\n",
        "\n",
        "This notebook runs the **USO Gradio Web App** in Google Colab with GPU acceleration.\n",
        "\n",
        "**USO** is a unified framework for style-driven and subject-driven image generation that can freely combine any subjects with any styles in any scenarios.\n",
        "\n",
        "## Features\n",
        "- **Subject/Identity-driven generation**: Place subjects into new scenes\n",
        "- **Style-driven generation**: Generate images matching a given style\n",
        "- **Style-subject driven generation**: Combine content and style references\n",
        "- **Multi-style generation**: Blend multiple style references\n",
        "\n",
        "---\n",
        "‚ö†Ô∏è **Important**: This notebook requires a GPU runtime. Go to **Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or higher)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 1. Mount Google Drive & Setup Environment\n",
        "\n",
        "First, we'll mount Google Drive to store model weights (which are ~75GB total), then clone the repository and install dependencies.\n",
        "\n",
        "**Why Google Drive?** Colab's local disk space is limited. By storing models on Drive, you:\n",
        "- Avoid re-downloading models every session\n",
        "- Have persistent storage for the large model files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create models folder in Google Drive if it doesn't exist\n",
        "import os\n",
        "DRIVE_MODELS_PATH = \"/content/drive/MyDrive/models\"\n",
        "os.makedirs(DRIVE_MODELS_PATH, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted! Models will be stored at: {DRIVE_MODELS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone the USO repository\n",
        "!git clone https://github.com/bytedance/USO.git\n",
        "%cd USO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# Install project dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Create symbolic link from local weights folder to Google Drive\n",
        "# Remove existing weights folder if it exists and create symlink\n",
        "!rm -rf ./weights\n",
        "!ln -s {DRIVE_MODELS_PATH} ./weights\n",
        "\n",
        "print(f\"‚úÖ Created symlink: ./weights -> {DRIVE_MODELS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf_token_section"
      },
      "source": [
        "## 2. Configure Hugging Face Token\n",
        "\n",
        "You need a Hugging Face token to download the FLUX.1-dev model weights.\n",
        "\n",
        "1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens) and create a token\n",
        "2. Accept the [FLUX.1-dev license](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
        "3. Enter your token below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_token_input"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Securely input your Hugging Face token\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Write the .env file with paths pointing to Google Drive (via symlink)\n",
        "env_content = f\"\"\"# Hugging face token\n",
        "HF_TOKEN={HF_TOKEN}\n",
        "\n",
        "# Core Flux weights\n",
        "FLUX_DEV=./weights/FLUX.1-dev/flux1-dev.safetensors\n",
        "FLUX_DEV_FP8=./weights/FLUX.1-dev/flux1-dev.safetensors\n",
        "AE=./weights/FLUX.1-dev/ae.safetensors\n",
        "\n",
        "# Text + vision encoders\n",
        "T5=./weights/t5-xxl\n",
        "CLIP=./weights/clip-vit-l14\n",
        "LORA=./weights/USO/uso_flux_v1.0/dit_lora.safetensors\n",
        "\n",
        "# USO LoRA + projector\n",
        "PROJECTION_MODEL=./weights/USO/uso_flux_v1.0/projector.safetensors\n",
        "SIGLIP_PATH=./weights/siglip\n",
        "\"\"\"\n",
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"‚úÖ .env file created successfully!\")\n",
        "print(f\"üìÅ Models will be stored in Google Drive: {DRIVE_MODELS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## 3. Download Model Weights\n",
        "\n",
        "This will download all necessary model weights to **Google Drive**. \n",
        "\n",
        "**First run:** May take 15-30 minutes depending on connection speed (~75GB total).  \n",
        "**Subsequent runs:** Models are already in Drive, so this step will be fast!\n",
        "\n",
        "**Models being downloaded:**\n",
        "- FLUX.1-dev (main diffusion model) - ~24GB\n",
        "- USO LoRA weights and projector - ~0.5GB\n",
        "- T5-XXL (text encoder) - ~44GB\n",
        "- CLIP ViT-L/14 (text encoder) - ~1.7GB\n",
        "- SigLIP (vision encoder) - ~3.5GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_weights"
      },
      "outputs": [],
      "source": [
        "# Check if models already exist in Google Drive\n",
        "import os\n",
        "\n",
        "def check_model_exists(path):\n",
        "    \"\"\"Check if a model file/folder exists and has content\"\"\"\n",
        "    full_path = os.path.join(DRIVE_MODELS_PATH, path)\n",
        "    if os.path.exists(full_path):\n",
        "        if os.path.isfile(full_path):\n",
        "            return os.path.getsize(full_path) > 0\n",
        "        else:\n",
        "            return len(os.listdir(full_path)) > 0\n",
        "    return False\n",
        "\n",
        "models_status = {\n",
        "    \"FLUX.1-dev\": check_model_exists(\"FLUX.1-dev/flux1-dev.safetensors\"),\n",
        "    \"USO\": check_model_exists(\"USO/uso_flux_v1.0\"),\n",
        "    \"T5-XXL\": check_model_exists(\"t5-xxl/pytorch_model.bin\"),\n",
        "    \"CLIP\": check_model_exists(\"clip-vit-l14/pytorch_model.bin\"),\n",
        "    \"SigLIP\": check_model_exists(\"siglip/model.safetensors\"),\n",
        "}\n",
        "\n",
        "print(\"üì¶ Model Status in Google Drive:\")\n",
        "for model, exists in models_status.items():\n",
        "    status = \"‚úÖ Found\" if exists else \"‚ùå Missing\"\n",
        "    print(f\"  {model}: {status}\")\n",
        "\n",
        "if all(models_status.values()):\n",
        "    print(\"\\nüéâ All models already exist! Skipping download.\")\n",
        "else:\n",
        "    print(\"\\n‚è¨ Downloading missing models...\")\n",
        "    !python ./weights/downloader.py\n",
        "    print(\"\\n‚úÖ All model weights downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "launch_section"
      },
      "source": [
        "## 4. Launch the Gradio Web App\n",
        "\n",
        "Now we'll launch the USO Gradio interface. The app will be accessible via a public URL.\n",
        "\n",
        "**Options:**\n",
        "- `--offload`: Enables sequential offloading of models to CPU when not in use (reduces VRAM usage)\n",
        "- `--name flux-dev-fp8`: Uses FP8 quantized model for lower memory usage (~16-18GB VRAM)\n",
        "\n",
        "For Colab T4 GPU (16GB VRAM), we use the memory-efficient configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_app"
      },
      "outputs": [],
      "source": [
        "# Launch the Gradio app with public sharing enabled\n",
        "# Using offload mode for lower VRAM usage on Colab\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Run the app in the background\n",
        "!python app.py --offload --name flux-dev-fp8 --port 7860 &\n",
        "\n",
        "# Wait for the server to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Create a public URL using localtunnel\n",
        "!npm install -g localtunnel\n",
        "!npx localtunnel --port 7860"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alternative_launch"
      },
      "source": [
        "### Alternative: Launch with Gradio's Built-in Sharing\n",
        "\n",
        "If localtunnel doesn't work, you can modify `app.py` to enable Gradio's built-in sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio_share"
      },
      "outputs": [],
      "source": [
        "# Alternative method: Enable Gradio sharing directly\n",
        "# This modifies app.py to use share=True\n",
        "\n",
        "# Read the original app.py\n",
        "with open(\"app.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Modify the launch line to include share=True\n",
        "content = content.replace(\n",
        "    \"demo.launch(server_port=args.port)\",\n",
        "    \"demo.launch(server_port=args.port, share=True)\"\n",
        ")\n",
        "\n",
        "# Write the modified app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ Modified app.py to enable public sharing\")\n",
        "print(\"\\nüöÄ Now run the cell below to start the app with a public Gradio URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_shared_app"
      },
      "outputs": [],
      "source": [
        "# Run the app with Gradio sharing enabled\n",
        "!python app.py --offload --name flux-dev-fp8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_tips"
      },
      "source": [
        "## üìù Usage Tips\n",
        "\n",
        "### Model Supports 3 Types of Usage:\n",
        "\n",
        "**1. Only Content Image (Subject/Identity-driven)**\n",
        "- Use natural prompts like \"A dog on the beach\" or \"The woman near the sea\"\n",
        "- For style editing: \"Transform the image into Ghibli style\"\n",
        "\n",
        "**2. Only Style Image (Style-driven)**\n",
        "- Upload one or more style reference images\n",
        "- Use any prompt to generate content in that style\n",
        "\n",
        "**3. Content + Style Images (Style-subject driven)**\n",
        "- Layout-preserved: Set prompt to **empty**\n",
        "- Layout-shifted: Use a natural prompt\n",
        "\n",
        "### Best Practices:\n",
        "- For portraits, use half-body close-ups for half-body prompts\n",
        "- Use full-body images when the pose changes significantly\n",
        "- The model is trained on 1024x1024 resolution\n",
        "\n",
        "---\n",
        "\n",
        "‚≠ê If you find USO helpful, please star the [GitHub repository](https://github.com/bytedance/USO)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM) Errors\n",
        "- Make sure you're using `--offload --name flux-dev-fp8` flags\n",
        "- Reduce the image resolution (e.g., 768x768 instead of 1024x1024)\n",
        "- Restart the runtime to clear GPU memory\n",
        "\n",
        "### Model Download Issues\n",
        "- Verify your Hugging Face token is correct\n",
        "- Make sure you've accepted the FLUX.1-dev license on Hugging Face\n",
        "- Check your internet connection\n",
        "\n",
        "### GPU Not Detected\n",
        "- Go to **Runtime ‚Üí Change runtime type ‚Üí GPU**\n",
        "- If no GPU is available, wait and try again later"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
